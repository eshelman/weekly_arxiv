# Automated Smart Contract Summarization via LLMs

Published: 2024-02-07T13:58:26Z
Updated: 2024-02-08T06:09:16Z

[Link to the paper](http://arxiv.org/abs/2402.04863v2)

## Authors

- Yingjie Mao
- Xiaoqi Li
- Zongwei Li
- Wenkai Li


## Summary
  Automatic code Summarization generation technology is widely used in the
development and maintenance of smart contracts. In recent years, with the
advent of Large Language Models (LLMs), Gemini has received a lot of attention
as the first Large Multimodal models (LMMs) to support multimodal input.
However, it is unclear how LMMs can generate contract code summarization from
multimodal inputs. In this paper, we focus on evaluating Gemini on real-world
smart contracts, comparing it to the MMTrans, and exploring how to combine
multimodal prompts to generate a contract code summarization. We used several
widely used metrics (BLEU, METEOR, and ROUGE-L) to measure the quality of the
generated summarization. Our experiments show that METEOR and ROUGEL metrics,
Gemini-Pro-Vision achieves 21.17% and 21.05% scores for code comments generated
by three-shot prompts. These scores are better than those generated by one-shot
and five-shot prompts.



# Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

Published: 2024-01-30T00:21:41Z
Updated: 2024-02-07T17:55:11Z

[Link to the paper](http://arxiv.org/abs/2402.01748v2)

## Authors

- Shengzhe Xu
- Christo Kurisummoottil Thomas
- Omar Hashash
- Nikhil Muralidhar
- Walid Saad
- Naren Ramakrishnan


## Summary
  Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language models that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the proposed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.



# Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot Learning

Published: 2020-05-19T05:28:35Z
Updated: 2024-02-06T09:21:28Z

[Link to the paper](http://arxiv.org/abs/2005.09218v2)

## Authors

- Jia-Fong Yeh
- Hsin-Ying Lee
- Bing-Chen Tsai
- Yi-Rong Chen
- Ping-Chia Huang
- Winston H. Hsu


## Summary
  In recent years, few-shot learning problems have received a lot of attention.
While methods in most previous works were trained and tested on datasets in one
single domain, cross-domain few-shot learning is a brand-new branch of few-shot
learning problems, where models handle datasets in different domains between
training and testing phases. In this paper, to solve the problem that the model
is pre-trained (meta-trained) on a single dataset while fine-tuned on datasets
in four different domains, including common objects, satellite images, and
medical images, we propose a novel large margin fine-tuning method (LMM-PQS),
which generates pseudo query images from support images and fine-tunes the
feature extraction modules with a large margin mechanism inspired by methods in
face recognition. According to the experiment results, LMM-PQS surpasses the
baseline models by a significant margin and demonstrates that our approach is
robust and can easily adapt pre-trained models to new domains with few data.



# PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology

Published: 2024-01-29T17:59:19Z
Updated: 2024-02-05T20:13:03Z

[Link to the paper](http://arxiv.org/abs/2401.16355v2)

## Authors

- Yuxuan Sun
- Hao Wu
- Chenglu Zhu
- Sunyi Zheng
- Qizi Chen
- Kai Zhang
- Yunlong Zhang
- Xiaoxiao Lan
- Mengyue Zheng
- Jingxiong Li
- Xinheng Lyu
- Tao Lin
- Lin Yang


## Summary
  The emergence of large multimodal models has unlocked remarkable potential in
AI, particularly in pathology. However, the lack of specialized, high-quality
benchmark impeded their development and precise evaluation. To address this, we
introduce PathMMU, the largest and highest-quality expert-validated pathology
benchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and
21,599 images from various sources, and an explanation for the correct answer
accompanies each question. The construction of PathMMU capitalizes on the
robust capabilities of GPT-4V, utilizing approximately 30,000 gathered
image-caption pairs to generate Q\&As. Significantly, to maximize PathMMU's
authority, we invite six pathologists to scrutinize each question under strict
standards in PathMMU's validation and test sets, while simultaneously setting
an expert-level performance benchmark for PathMMU. We conduct extensive
evaluations, including zero-shot assessments of 14 open-sourced and three
closed-sourced LMMs and their robustness to image corruption. We also fine-tune
representative LMMs to assess their adaptability to PathMMU. The empirical
findings indicate that advanced LMMs struggle with the challenging PathMMU
benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\%
zero-shot performance, significantly lower than the 71.4\% demonstrated by
human pathologists. After fine-tuning, even open-sourced LMMs can surpass
GPT-4V with a performance of over 60\%, but still fall short of the expertise
shown by pathologists. We hope that the PathMMU will offer valuable insights
and foster the development of more specialized, next-generation LLMs for
pathology.



# Generalizable Entity Grounding via Assistance of Large Language Model

Published: 2024-02-04T16:06:05Z

[Link to the paper](http://arxiv.org/abs/2402.02555v1)

## Authors

- Lu Qi
- Yi-Wen Chen
- Lehan Yang
- Tiancheng Shen
- Xiangtai Li
- Weidong Guo
- Yu Xu
- Ming-Hsuan Yang


## Summary
  In this work, we propose a novel approach to densely ground visual entities
from a long caption. We leverage a large multimodal model (LMM) to extract
semantic nouns, a class-agnostic segmentation model to generate entity-level
segmentation, and the proposed multi-modal feature fusion module to associate
each semantic noun with its corresponding segmentation mask. Additionally, we
introduce a strategy of encoding entity segmentation masks into a colormap,
enabling the preservation of fine-grained predictions from features of
high-resolution masks. This approach allows us to extract visual features from
low-resolution images using the CLIP vision encoder in the LMM, which is more
computationally efficient than existing approaches that use an additional
encoder for high-resolution images. Our comprehensive experiments demonstrate
the superiority of our method, outperforming state-of-the-art techniques on
three tasks, including panoptic narrative grounding, referring expression
segmentation, and panoptic segmentation.



